<!-- Source: https://observablehq.com/@vega/vega-lite-api#standalone_use -->
<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<!-- Vega-Lite -->
		<script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
		<script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
		<script src="https://cdn.jsdelivr.net/npm/vega-lite-api@5.6.0"></script>
		<script src="https://cdn.jsdelivr.net/npm/vega-tooltip"></script>

		<!-- D3 -->
		<script src="https://d3js.org/d3.v7.min.js"></script>
		<script src="https://cdn.jsdelivr.net/npm/d3-dispatch@3"></script>
		<script src="https://cdn.jsdelivr.net/npm/d3-quadtree@3"></script>
		<script src="https://cdn.jsdelivr.net/npm/d3-timer@3"></script>
		<script src="https://cdn.jsdelivr.net/npm/d3-force@3"></script>

		<!-- Fonts -->
		<link rel="preconnect" href="https://fonts.googleapis.com" />
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
		<link
			href="https://fonts.googleapis.com/css2?family=Fredericka+the+Great&family=Patrick+Hand&family=Patrick+Hand+SC&family=Sue+Ellen+Francisco&display=swap"
			rel="stylesheet"
		/>

		<!-- Auto-reloads the page on edit -->
		<script type="text/javascript" src="https://livejs.com/live.js"></script>

		<!-- My files -->
		<script src="./main.js"></script>
		<link rel="stylesheet" type="text/css" href="style.css" />

		<title>Algorithmic Fairness: An Explorable Explanation</title>
	</head>

	<body>
		<section class="sectionHeader" id="introHeader">
			<h1>Algorithmic Fairness</h1>
			<h2>An Explorable Explanation</h2>
			<a name="introduction"></a>
		</section>

		<section>
			<p>
				Computers is taking over the world. And that's pretty cool, because
				machines are completely impartial, which means that they must be super
				fair... right?
			</p>

			<p>
				Wrong. Computers learn patterns and apply them: this is called an
				<span class="vocabWord">algorithm</span>. And
				<strong
					>if the patterns it learns are unfair or biased, then the algorithm
					will be unfair and biased</strong
				>, too.
			</p>

			<p>
				Algorithms is really complicated, so it's hard to tell when the computer
				is being unfair. But we can look at the patterns it produces and see if
				there are any trends we don't like.
			</p>

			<p>Let's look at a real example.</p>
		</section>

		<section class="sectionHeader">
			<h1>Predicting Recidivism</h1>
			<a name="predicting_recidivism"></a>
		</section>

		<section>
			<p>
				If Alice is suspected of committing a crime, she'll be put on trial. To
				make sure that she'll show up for her trial and accept the ruling, the
				government will keep her in jail until the trial.
			</p>

			<p>
				It's expensive to keep people in jail, though, so the US has a process
				called bail. Alice can give the government money to hold on to until her
				trial. She's betting that she will attend: if she comes to the trial,
				she gets the money back, but if she skips town, she loses the money.
			</p>

			<p>
				Importantly, the government should only take this bet if they're
				confident Alice won't harm society while she's free. If she's a
				supervillainess, keeping her in jail until her trial is the safest thing
				to do. If she just made a one-time mistake, though, it's just as safe to
				let her go - and cheaper for the government.
			</p>

			<p>
				So,
				<strong
					>we want a way to predict if a person is going to commit more
					crimes</strong
				>. This is called <span class="vocabWord">recidivism</span>. There are a
				lot of factors we could use to predict recidivism: if they've skipped
				town before, or how many other crimes they've committed, or how polite
				they are.
			</p>

			<p>
				Having a human judge predict if someone is going to recidivate or not
				leaves a lot of room for bias. Different judges weigh different factors
				more heavily, consciously or unconsciously. The decision may be affected
				by race or gender or the time a judge last ate. That doesn't seem very
				fair (and also judges are expensive).
			</p>

			<p>
				We can use an algorithm to make a more objective evaluation: we can
				ensure it doesn't see Alice's race or gender, we can provide consistent
				weights for factors, and the algorithm will never be hungry. In 2013,
				Broward County, Florida, introduced an algorithm called COMPAS to
				estimate "risk scores."
				<strong>How fair was their algorithm?</strong>
			</p>
		</section>

		<section class="sectionHeader">
			<!-- TODO: make these sections have easier names. or say the names in the body of the text -->
			<h1>Anti-Classification</h1>
			<a name="anti_classification"></a>
		</section>

		<section>
			<p>
				Both humans and computers will use information about Alice to predict if
				she will recidivate or not. This might make sense for some information
				(someone who's had many prior crimes may be more likely to commit future
				crimes), but less sense for others (it's probably irrelevant that their
				name begins with a vowel).
			</p>

			<p>
				It feels like there are some pieces of information about Alice that we
				<em>don't</em> want to use to make this prediction, for the sake of
				fairness. Justice is blind, after all; it is wrong (and also illegal)
				for people to be jailed on the basis of their sex or race or other
				things beyond their control.
			</p>

			<p>
				In the legal system, this kind of information is called a
				<span class="vocabWord">protected characteristic</span>. Generally,
				protected characteristics include things like race, sex, age, disability
				status, marital status, religion, and sexual orientation.
			</p>

			<p>
				COMPAS, the algorithm used in Florida, did not include race in its risk
				score calculations.
				<strong
					>If COMPAS is not racially biased, then we shouldn't see vastly
					different outcomes for people of different races</strong
				>.
			</p>

			<p>
				Let's take two people, Bob and Carina. Bob and Carina have identical
				criminal records and are accused of committing the exact same crime. The
				only difference between them is their race. If their race isn't taken
				into account, then they should receive the same risk score. But that's
				not what happens with COMPAS.
			</p>

			<div class="vizInstructions">
				<p>
					In the chart below, every dot represents a person who was evaluated by
					COMPAS. You can use the dropdown to group people based on a
					characteristic, and compare the distribution of scores for the
					different groups.
				</p>
				<p>
					Some of these characteristics are probably helpful for predicting
					recidivism. Some are probably not. With relevant characteristics, how
					similar are the groups? With irrelevant characteristics? What about
					protected characteristics?
				</p>
			</div>

			<div class="container">
				<div class="chartWrapper" id="antiClassification1"></div>
			</div>

			<!-- TODO: finish adding options -->
			<div class="container">
				<select name="antiClassification2select" id="antiClassification2select">
					<option value="all">All</option>
					<option value="race">Race</option>
					<option value="sex">Sex</option>
					<option value="name_begins_with_vowel">Name Begins With...</option>
					<option value="charge_degree">Charge Degree</option>
					<option value="day_of_week">Born on a...</option>
					<option value="age_cat">Age</option>
				</select>
			</div>

			<div class="container" id="antiClassification2"></div>

			<p>
				What we can see here is that different races are receiving very
				different risk scores:
				<strong
					>Black individuals tend to receive high risk scores, while white
					individuals tend to receive lower risk scores.</strong
				>
			</p>

			<p>
				That doesn't seem very fair! Irrelevant groupings (like "name begins
				with vowel") have roughly the same distribution of risk scores, but
				grouping by race paints a very different picture. What's up with that??
			</p>
		</section>

		<section class="sectionHeader">
			<h1>Calibration</h1>
			<a name="calibration"></a>
		</section>

		<section>
			<p>
				Mathematically speaking, groupings by characteristic should have roughly
				the same distribution as the ungrouped whole,
				<em
					>unless the groups are based on a characteristic that affects the risk
					score</em
				>. But COMPAS doesn't know the person's race!
			</p>

			<p>
				We can conclude, then, that race must affect the risk score, which means
				that race affects an individual's likelihood of recidivism. And
				recidivism, remember, is being measured here as being charged with a new
				crime. The systemic racism of the American criminal justice system means
				that Black individuals are charged with crimes at higher rates than
				white individuals. Because the bias of who gets evaluated by the
				algorithm,
				<strong
					>even if the system is fair, we shouldn't expect to see matching
					distributions of risk scores</strong
				>. What's a better way to see if COMPAS is fair?
			</p>

			<p>
				Even if the specific outcomes are different for different groups, we can
				look at how often the algorithm is right or wrong.
				<strong
					>We want risk scores to predict recidivism equally well for all
					groups</strong
				>. The score are designed so that a 1 corresponds to 10% chance of
				recidivating, at 2 to 20%, and so on.
			</p>

			<p>
				In a fair system, if Daphne and Ed have the same risk score, they should
				have the same likelihood of recidivating, no matter what their race is.
			</p>

			<div class="vizInstructions">
				<p>something here about accuracy idk</p>
			</div>

			<div class="container">
				<select name="calibrationSelect" id="calibrationSelect">
					<option value="all">All</option>
					<option value="race">Race</option>
					<option value="sex">Sex</option>
					<option value="name_begins_with_vowel">Name Begins With...</option>
					<option value="charge_degree">Charge Degree</option>
					<option value="day_of_week">Born on a...</option>
					<option value="age_cat">Age</option>
				</select>
			</div>

			<div class="container" id="calibration"></div>
		</section>

		<section class="sectionHeader">
			<h1>Confusion Matrix</h1>
			<a name="confusion_matrix"></a>
		</section>

		<section>
			<p>Well, those lines are pretty darn close. Great, we solved it!</p>
			<p>Or did we? Let's see how COMPAS plays out.</p>

			<div class="vizInstructions">
				<p>
					Sometimes the algorithm is right, and jails those who would have
					recidivated. But it also sometimes jails those who wouldn't have. We
					want the risk scores to predict recidivism equally well: the same
					proportion of people <em>predicted</em> to not recidivate who
					<em>actually</em> don't recidivate should be the same across groups.
				</p>

				<p>disclaimer about sample size</p>

				<p>
					How many people are being wrongly kept in jail (top right)? How many
					people are out on bail and commit another crime (bottom left)?
				</p>
			</div>

			<div class="container">
				<!-- TODO: set this to trigger only when scrolled into view -->
				<select name="confusionMatrixSelect" id="confusionMatrixSelect">
					<option value="all">All</option>
					<option value="race">Race</option>
					<option value="sex">Sex</option>
					<option value="name_begins_with_vowel">Name Begins With...</option>
					<option value="charge_degree">Charge Degree</option>
					<option value="day_of_week">Born on a...</option>
					<option value="age_cat">Age</option>
				</select>
			</div>

			<div class="container" id="confusionMatrix"></div>

			<p>
				So, while COMPAS is equally (in)accurate for both groups, the
				<em>kind</em> of mistakes it makes are different for each group.
				Mistakes for Black individuals lead to unnecessary imprisonment, while
				mistakes for white individuals lead to more crimes being committed.
				What's up with that???
			</p>
		</section>

		<section class="sectionHeader">
			<h1>Infra-Marginality</h1>
			<a name="inframarginality"></a>
		</section>

		<p>We want to treat people with the same risk score in the same way.</p>

		<section>
			<div class="container">
				<select name="inframarginalitySelect" id="inframarginalitySelect">
					<option value="all">All</option>
					<option value="race">Race</option>
					<option value="sex">Sex</option>
					<option value="name_begins_with_vowel">Name Begins With...</option>
					<option value="charge_degree">Charge Degree</option>
					<option value="day_of_week">Born on a...</option>
					<option value="age_cat">Age</option>
				</select>
			</div>

			<div class="container" id="inframarginality"></div>
		</section>

		<section class="sectionHeader">
			<h1>Conclusion</h1>
			<a name="conclusion"></a>
		</section>

		<section>
			<p>
				An algorithm is not impartial. Humans design the algorithm, and encode
				their own values into it. It's up to us to look at the outcomes of an
				algorithm and decide if we like what we see. We can always change the
				algorithm to align with our values - if we decide and define what our
				values are.
			</p>
		</section>

		<section id="footerSection">
			<p>Thanks to so and so for blah blah.</p>
		</section>
	</body>
</html>
